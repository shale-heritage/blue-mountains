# Blue Mountains Reports Directory

This directory contains human-readable reports generated by analysis scripts. Reports are written in Markdown format for readability and are excluded from Git (.gitignore).

## Report Philosophy

Reports are designed for **human review and decision-making**, not machine processing. They:

- Summarise key findings in plain language
- Highlight actionable recommendations
- Provide context for quantitative results
- Support manual curation workflows

**Note:** This directory should be empty in a fresh clone. Run scripts to generate reports.

## Report Inventory

| Report | Generated By | Audience | Update Frequency |
|--------|--------------|----------|------------------|
| `tag_summary.md` | 01_extract_tags.py | Project team, historians | After each extraction |
| `tag_analysis.md` | 02_analyze_tags.py | Vocabulary developers | After tag analysis |
| `data_quality_issues.md` | 02_analyze_tags.py | Data curators | After quality checks |
| `multiple_attachments_inspection.md` | 03_inspect_multiple_attachments.py | Research assistants | After attachment inspection |

## Report Descriptions

### tag_summary.md

**Purpose:** Overview statistics from tag extraction

**Generated by:** `python scripts/01_extract_tags.py`

**Contains:**

- Library statistics (total items, tagged items, unique tags)
- Tag frequency distribution (top 20 most/least common tags)
- Tagging coverage (percentage of items with tags)
- Tags per item statistics (average, min, max)
- Items without tags (flagged for review)

**Intended audience:**

- Project team: Quick health check of tagging progress
- Historians: Understand subject coverage
- Research assistants: Identify items needing tags

**How to interpret:**

- **High unique_tags count:** Rich subject coverage (good) or inconsistent terminology (needs consolidation)
- **Low items_with_tags percentage:** More tagging work needed
- **High avg_tags_per_item:** Detailed subject indexing (good) or over-tagging (review)

**Typical review frequency:** Weekly during active tagging, monthly during analysis phase

---

### tag_analysis.md

**Purpose:** Detailed tag pattern analysis with consolidation recommendations

**Generated by:** `python scripts/02_analyze_tags.py`

**Contains:**

- Similar tags (fuzzy matching results with similarity scores)
- Hierarchical relationships (detected parent-child tags)
- Co-occurrence patterns (tags frequently used together)
- Tag network metrics (centrality, clustering)
- Consolidation recommendations

**Intended audience:**

- Vocabulary developers: Make consolidation decisions
- Project PIs: Review subject taxonomy
- Data curators: Plan controlled vocabulary structure

**How to interpret:**

- **Similar tags section:** Review pairs with similarity ≥80% for potential merging
  - Consider: Are these true variants or distinct concepts?
  - Action: Merge in Zotero or keep separate with clear definitions
- **Hierarchies section:** Identify broader/narrower relationships for SKOS
  - Example: "Mining" (broader) → "Coal Mine" (narrower)
  - Action: Structure controlled vocabulary with skos:broader/narrower
- **Co-occurrence section:** Tags used together suggest related concepts
  - Example: "Mining" + "Katoomba" = geographic association
  - Action: Consider creating compound terms or cross-references

**Typical review frequency:** After major tagging milestones, before vocabulary publication

---

### data_quality_issues.md

**Purpose:** Flagged items requiring manual review and curation

**Generated by:** `python scripts/02_analyze_tags.py`

**Contains:**

- Potential duplicates (items with identical/similar titles)
- Non-primary sources (reference works to exclude from analysis)
- Items with multiple attachments (may combine distinct sources)
- Items without attachments (missing PDFs)
- Summary statistics for each category

**Intended audience:**

- Data curators: Prioritise cleanup tasks
- Research assistants: Address flagged items
- Project managers: Track data quality metrics

**How to interpret:**

- **Duplicates:** Check if items are truly identical
  - Action: Merge duplicates in Zotero, preserve metadata
- **Non-primary sources:** Verify categorisation
  - Action: Move to separate collection or delete if not needed
- **Multiple attachments:** See multiple_attachments_inspection.md for details
  - Action: Review attachment patterns, split if needed
- **No attachments:** Identify missing sources
  - Action: Locate PDFs, upload to Zotero

**Priority levels:**

- HIGH: Duplicates, multiple attachments with distinct sources
- MEDIUM: Non-primary sources review, missing attachments
- LOW: Items with PDF + notes (likely legitimate)

**Typical review frequency:** Monthly during curation, before publication

---

### multiple_attachments_inspection.md

**Purpose:** Detailed inspection of items with multiple attachments, categorised by pattern

**Generated by:** `python scripts/03_inspect_multiple_attachments.py`

**Contains:**

- Categorised items (multiple PDFs, PDF+notes, multiple notes, mixed, uncertain)
- Priority levels (HIGH/MEDIUM/LOW)
- Attachment details (filenames, types, counts)
- Recommended actions for each item

**Intended audience:**

- Research assistants: Conduct manual review
- Project PIs: Make splitting decisions
- Data curators: Implement changes

**How to interpret:**

- **HIGH PRIORITY - Multiple PDFs:** Likely distinct articles combined
  - Action: Open PDFs, check if separate sources, split into individual items
- **LOW PRIORITY - PDF + Notes:** Likely text extraction from PDF
  - Action: Verify notes match PDF content, keep together
- **REVIEW - Multiple Notes:** May be sections that should be consolidated
  - Action: Check if notes should merge into single note
- **REVIEW - Mixed Content:** Unusual pattern needs investigation
  - Action: Manual inspection to understand attachment purpose

**Decision framework:**

- If attachments are different sources → Split into separate items
- If attachments are related materials (notes, images) → Keep together
- If uncertain → Consult with project team

**Typical review frequency:** As needed when quality_multiple_attachments.csv has entries

## Report Format

### Markdown Structure

All reports use Markdown with consistent formatting:

- **Headings:** `#` for title, `##` for sections, `###` for subsections
- **Lists:** Bulleted for qualitative points, numbered for procedures
- **Tables:** Pipe-delimited for structured data
- **Code blocks:** For data examples, commands
- **Emphasis:** *Italic* for terms, **bold** for important notes

### Reading in Different Tools

**GitHub/GitLab:**

- Markdown renders automatically in web interface
- Tables, lists, and formatting display correctly

**VS Code:**

- Install Markdown Preview extension
- Right-click file → "Open Preview" (Ctrl+Shift+V)

**Command line:**

- Use `less reports/tag_summary.md` for basic reading
- Install `mdcat` or `glow` for formatted terminal rendering:

  ```bash
  # Debian/Ubuntu
  sudo apt install glow
  glow reports/tag_summary.md
  ```

**Convert to other formats:**

- **PDF:** Use pandoc:

  ```bash
  pandoc reports/tag_analysis.md -o tag_analysis.pdf
  ```

- **HTML:** Use pandoc or markdown-to-html tools
- **Word:** Open in Typora, export as .docx

### Generation Timestamps

All reports include generation timestamp at top:

```markdown
# Tag Analysis Report

**Generated:** 2025-10-09 10:45:00 AEDT
**Source Data:** data/raw_tags.json (generated 2025-10-09 10:30:00)
```

Check timestamps to ensure reports are current with latest data.

## Interpretation Guidelines

### Statistical Significance

**Tag frequencies:**

- Tags used <3 times: Likely too specific or errors (review for deletion)
- Tags used 3-10 times: Specific topics (keep if relevant)
- Tags used >20 times: Core subjects (high priority for vocabulary)

**Similarity scores (0-100):**

- 95-100: Almost certainly variants (merge unless intentional distinction)
- 85-94: Likely variants (review for merging)
- 80-84: Possibly related (manual judgment required)
- <80: Not reported (dissimilar)

**Co-occurrence counts:**

- Count ≥10: Strong association (consider related terms)
- Count 5-9: Moderate association (review context)
- Count 3-4: Weak association (may be coincidental)
- Count <3: Not reported (insufficient evidence)

### Context Matters

Numbers alone don't tell the full story. Consider:

1. **Domain knowledge:** Do similar tags represent distinct concepts in mining history?
2. **User intent:** Were tags meant to be hierarchical or flat?
3. **Workflow stage:** Early tagging may be exploratory, later tagging more systematic
4. **Team coordination:** Are multiple researchers using different terminology?

### When in Doubt

- **Consult project team:** Schedule review meeting with historians and PIs
- **Check item examples:** Read actual item titles/content to understand tag usage
- **Document decisions:** Add notes to planning/ folder explaining vocabulary choices
- **Iterate:** Controlled vocabulary development is iterative, not one-time

## Action Workflows

### After Reviewing tag_summary.md

1. **If many untagged items:**
   - Schedule tagging sessions
   - Assign items to research assistants
   - Consider AI-assisted tagging (Phase 2)

2. **If tag diversity is high:**
   - Proceed to 02_analyze_tags.py for consolidation analysis
   - Plan vocabulary development workshop

### After Reviewing tag_analysis.md

1. **Similar tags identified:**
   - Export similar_tags.csv to Excel
   - Review each pair, mark "merge" or "keep separate"
   - Apply merges in Zotero manually or via script
   - Rerun 01_extract_tags.py to verify

2. **Hierarchies detected:**
   - Map to SKOS broader/narrower relationships
   - Begin controlled vocabulary structure development
   - Document hierarchy decisions

3. **Co-occurrences noted:**
   - Identify cross-disciplinary tags (mining + environment, mining + gender)
   - Consider creating faceted vocabulary
   - Plan compound term creation if needed

### After Reviewing data_quality_issues.md

1. **Duplicates flagged:**
   - Open each pair in Zotero
   - Merge if truly duplicate
   - Keep separate if distinct items with similar titles
   - Document merges in planning/data-quality-log.md

2. **Non-primary sources flagged:**
   - Verify categorisation correct
   - Move to "Reference Materials" collection in Zotero
   - Exclude from primary source analysis

3. **Multiple attachments flagged:**
   - Run 03_inspect_multiple_attachments.py for detailed review
   - Follow multiple_attachments_inspection.md recommendations

4. **Missing attachments flagged:**
   - Cross-reference with archival sources (Trove, NLA)
   - Prioritise digitisation of high-value items
   - Upload PDFs to Zotero when located

### After Reviewing multiple_attachments_inspection.md

1. **HIGH PRIORITY items:**
   - Review immediately (likely distinct sources combined)
   - Split into separate Zotero items if needed
   - Preserve metadata (tags, notes) on appropriate split item

2. **REVIEW items:**
   - Schedule for next curation session
   - Document decision (split or keep together)

3. **LOW PRIORITY items:**
   - Spot-check a few to verify categorisation correct
   - Can defer detailed review

## See Also

- **data/README.md:** Data files that feed into these reports
- **scripts/README.md:** How to regenerate reports
- **CONTRIBUTING.md:** Report quality standards
- **planning/:** Decision logs and vocabulary development notes

---

## Questions?

- **Report unclear:** Open GitHub issue with specific questions
- **Recommendations uncertain:** Schedule team review meeting
- **Technical issues:** Check scripts/README.md troubleshooting section

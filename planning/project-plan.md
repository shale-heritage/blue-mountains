# Zotero â†’ Omeka Classic Publication Workflow

**Project:** Historical Newspaper Sources + Archaeological Evidence Digital Collection
**Date Created:** 2025-10-08
**Last Updated:** 2025-10-10
**Tools:** Zotero API, Omeka Classic API, Claude/Local LLM, CurateScape

---

## Current Status (2025-10-10)

**Phase 1:** ðŸ”„ In Progress - Tag rationalisation infrastructure complete, analysis ongoing

**Recent Achievements:**
- âœ… Complete FAIR4RS documentation implementation (Phases A-E, Oct 9-10, 2025)
- âœ… Core analysis scripts developed and documented (01_extract_tags.py, 02_analyze_tags.py, 03_inspect_multiple_attachments.py)
- âœ… Comprehensive code documentation (~4,100 lines)
- âœ… Project metadata and compliance files (CITATION.cff, codemeta.json, CONTRIBUTING.md)
- âœ… Technical documentation (API integration, data formats, vocabularies, gazetteers)
- âœ… Folder-specific READMEs for all major directories
- âœ… Quality assurance complete (Python linting, markdown linting, UK/Australian spelling)
- âœ… Configuration updated for Omeka Classic integration (shaleheritage.au)

**Documentation Statistics:**
- 29 markdown files
- ~16,200 lines of documentation
- 16:1 documentation-to-code ratio
- 0 linting errors across all files
- Publication-ready research software

**Next Steps:**
- Complete Phase 1.1: Historian consultation on tag folksonomy
- Begin Phase 1.2: Tag schema rationalisation
- Develop Phase 1.3: Getty and RVA vocabulary mapping

---

## Phase 1: Tag Rationalization & Vocabulary Publishing ðŸ”„ IN PROGRESS

### 1.1 Folksonomy Analysis âœ… INFRASTRUCTURE COMPLETE

**Completed:**
- âœ… Extract all existing tags from Zotero group library (01_extract_tags.py)
- âœ… Analyse tag usage patterns, frequencies, overlaps (02_analyze_tags.py)
  - Fuzzy matching for similar tags (80% similarity threshold)
  - Hierarchical relationship detection (substring analysis)
  - Co-occurrence network analysis
  - Data quality categorisation
- âœ… Generate comprehensive analysis reports:
  - tag_summary.md - Overview statistics
  - tag_analysis.md - Detailed pattern analysis
  - similar_tags.csv - Consolidation suggestions
  - tag_hierarchy.csv - Parent/child relationships
  - tag_cooccurrence.csv - Network relationships

**Remaining:**
- ðŸ”„ Interview/consult with historians who created tags
- ðŸ”„ Document current folksonomy logic and categories

**Scripts:** 01_extract_tags.py (1,420 lines), 02_analyze_tags.py (2,164 lines)

### 1.2 Tag Schema Rationalization ðŸ“‹ PLANNED

**Tasks:**
- Consolidate similar/duplicate tags (using similar_tags.csv recommendations)
- Standardise terminology and structure
- Create hierarchical organisation if needed (using tag_hierarchy.csv)
- Document definitions and scope notes

**Dependencies:** Requires historian consultation (Phase 1.1)

### 1.3 Vocabulary Mapping & Publication ðŸ“‹ PLANNED

**Tasks:**
- Map rationalised tags â†’ Getty vocabularies (AAT/TGN)
- Prepare metadata for Research Vocabularies Australia (RVA)
- Publish controlled vocabulary to RVA
- Document mappings for interoperability

**Documentation Available:**
- docs/vocabularies.md - Getty AAT, TGN, and RVA standards
- docs/gazetteer-comparison.md - Geographic vocabulary options

**Dependencies:** Requires Phase 1.2 completion

### 1.4 Batch Update Zotero ðŸ“‹ PLANNED

**Tasks:**
- Apply rationalised tags to existing items via API
- Preserve original tags as backup (separate collection/export)
- Validate updates

**Documentation Available:** docs/api-integration.md - Zotero API patterns

**Dependencies:** Requires Phase 1.3 completion

## Phase 2: AI-Assisted Tagging (Terminal-Based)

### 2.1 Content Extraction
- Pull untagged primary sources from Zotero API
- Extract text from PDFs/notes field
- Prepare batches for analysis

### 2.2 AI Tagging Options
**Option A: Claude Code in terminal (this session)**
- Process PDFs/text directly here
- Apply controlled vocabulary interactively
- Update Zotero via API in batches

**Option B: Local LLM (120B)**
- Script to feed content to local model
- Prompt engineering with controlled vocabulary
- Batch processing for efficiency

### 2.3 Validation Workflow
- Intensive manual review of first 5-10 PDFs
- Refine prompts/model based on results
- Spot-check 5-10% of remaining items
- Statistical validation across corpus

## Phase 3: Location Data Enhancement

### 3.1 Location Extraction
- Parse articles for place names
- Match to existing location metadata
- Geocode locations (use Getty TGN where mapped)
- Handle imprecise locations ("near X", "region of Y")

### 3.2 CurateScape Location Setup
- Add lat/long to Omeka items
- Group items by geographic proximity
- Link newspaper sources to archaeological sites
- Create location clusters for tours

## Phase 4: Omeka Classic Publication (FAIR-Compliant)

### 4.1 Metadata Transformation
- Map Zotero fields â†’ Dublin Core
- Include:
  - Controlled vocabulary tags
  - Getty mappings
  - RVA vocabulary URIs
  - Location coordinates
  - Trove source URLs (persistent IDs)

### 4.2 License Management
- Apply CC-BY by default
- Inherit/respect Trove licenses per item
- Document license in metadata
- Include rights statements in exhibits

### 4.3 Automated Publishing Pipeline
- `zotero_to_omeka.py` script:
  - Pull tagged items from Zotero group
  - Transform metadata + PDFs
  - POST to Omeka Classic API
  - Assign to Collections (by theme/tag)
  - Set geographic data for CurateScape

### 4.4 Collections & Exhibits
- Create Collections based on tag groupings
- Build Exhibits combining sources + context
- Prepare for CurateScape tour integration

## Phase 5: CurateScape Tours

### 5.1 Tour Design
- Group items by location + theme
- Combine newspaper articles + archaeological evidence
- Create narrative structures
- Test mobile functionality

### 5.2 Integration
- Configure CurateScape with published items
- Set up location-based triggers
- Build interpretive content
- QA on iOS/Android apps

## Phase 6: Archaeological Integration (Future)

- Match DSLR images to records
- Apply compatible tag schema
- Import to Omeka via API
- Link to newspaper sources via tags + locations
- Expand tours with material evidence

---

## Key Scripts: Development Status

### Core Infrastructure âœ… COMPLETE

1. **`config.py`** âœ… - Configuration management (296 lines)
   - Environment variable handling
   - Zotero and Omeka API configuration
   - Path management and validation
   - Security-focused design

2. **`01_extract_tags.py`** âœ… - Tag extraction from Zotero (1,420 lines)
   - Complete Zotero API integration
   - Tag frequency analysis
   - Item association tracking
   - Comprehensive reporting

3. **`02_analyze_tags.py`** âœ… - Tag pattern analysis (2,164 lines)
   - Fuzzy matching for similarity detection
   - Hierarchical relationship detection
   - Co-occurrence network analysis
   - Data quality categorisation
   - Network visualisation

4. **`03_inspect_multiple_attachments.py`** âœ… - Data quality checks (1,243 lines)
   - Multiple attachment detection and categorisation
   - Priority-based flagging system
   - Detailed inspection reports

### Phase 1 Scripts ðŸ“‹ PLANNED

5. **`vocabulary_mapper.py`** ðŸ“‹ - Getty/RVA mapping tools
   - Map tags to Getty AAT (Art & Architecture Thesaurus)
   - Map locations to Getty TGN (Thesaurus of Geographic Names)
   - Prepare SKOS format for RVA publication
   - Validate mappings

### Phase 2-3 Scripts ðŸ“‹ PLANNED

6. **`ai_tagger_claude.py`** ðŸ“‹ - Terminal-based tagging with Claude Code
   - Content extraction from PDFs
   - LLM-based tag suggestion
   - Interactive validation workflow

7. **`ai_tagger_local.py`** ðŸ“‹ - Alternative using local LLM
   - Batch processing for untagged items
   - Local model integration (up to 120B)

8. **`location_extractor.py`** ðŸ“‹ - NER for place names + geocoding
   - Named entity recognition for locations
   - Getty TGN mapping
   - Coordinate assignment

### Phase 4 Scripts ðŸ“‹ PLANNED

9. **`license_handler.py`** ðŸ“‹ - Trove licence detection + CC-BY application
    - Parse Trove licence metadata
    - Apply CC-BY as default
    - Document licence provenance

10. **`zotero_to_omeka.py`** ðŸ“‹ - Main publication pipeline
    - Metadata transformation (Zotero â†’ Dublin Core)
    - Item creation via Omeka Classic API
    - Collection assignment
    - Geographic data integration for CurateScape

11. **`fair_validator.py`** ðŸ“‹ - Metadata quality checks
    - Validate FAIR4RS compliance
    - Check required metadata fields
    - Verify controlled vocabulary usage
    - Generate quality reports

---

## Deliverables

**Academic:**
- âœ… Reproducible scripts + comprehensive documentation (FAIR4RS compliant, Oct 2025)
- ðŸ“‹ Rationalised controlled vocabulary published to RVA
- ðŸ“‹ Mappings to Getty vocabularies
- ðŸ“‹ Fully-tagged Zotero group library (shareable)

**Public-Facing:**
- ðŸ“‹ FAIR-compliant Omeka Classic collection
- ðŸ“‹ CurateScape mobile tours
- ðŸ“‹ Thematic exhibits

**Documentation (Completed Oct 2025):**
- âœ… CITATION.cff, codemeta.json for software citation
- âœ… Comprehensive README.md (667 lines)
- âœ… API integration guide (docs/api-integration.md, 800 lines)
- âœ… Data format specifications (docs/data-formats.md)
- âœ… Vocabulary standards documentation (docs/vocabularies.md)
- âœ… Gazetteer comparison (docs/gazetteer-comparison.md)
- âœ… Folder-specific READMEs (scripts, data, reports, planning)
- âœ… Code documentation (~4,100 lines of docstrings and comments)
- âœ… CONTRIBUTING.md, CHANGELOG.md, CLAUDE.md

---

## Project Configuration

**Zotero:**
- Type: Group library (ID: 2258643)
- API: Enabled and documented (docs/api-integration.md)
- Focus: Primary sources (newspaper articles, late 19th/early 20th century)
- Status: âœ… Configured in scripts/config.py

**Omeka:**
- Version: Omeka Classic
- Site: <https://shaleheritage.au/>
- API Endpoint: <https://shaleheritage.au/api>
- Maximum results per page: 50
- API: Enabled and documented (docs/api-integration.md)
- Extensions: CurateScape for mobile tours
- Status: âœ… Configured in scripts/config.py and .env.example

**Licensing:**
- Default: CC-BY
- Trove items: Inherit original licences
- Documentation: README.md, CONTRIBUTING.md

**AI Tagging:**
- Primary: Claude Code (terminal-based)
- Alternative: Local LLM (up to 120B)
- Validation: 5-10% spot-checking after initial intensive review

---

## Optional Future Enhancements

Beyond the core 6-phase workflow, optional enhancements could improve software sustainability and reach:

### High Priority (Low Effort)

1. **Zenodo DOI Integration** (2-3 hours) - Persistent identifiers for software releases
2. **GitHub Actions CI** (3-5 hours) - Automated testing and quality checks
3. **Docker Container** (4-6 hours) - Simplified installation and deployment

### Medium Priority (Medium Effort)

4. **Example Jupyter Notebooks** (6-8 hours) - Interactive tutorials for researchers
5. **Command-Line Interface** (6-10 hours) - Unified CLI for all scripts
6. **Automated Testing Suite** (10-15 hours) - pytest framework with >80% coverage

### Lower Priority (Higher Effort)

7. **Read the Docs Hosting** (8-12 hours) - Professional documentation with versioning
8. **Data Quality Dashboard** (12-18 hours) - Visual analytics using Dash or Streamlit
9. **Video Tutorials** (10-15 hours) - Installation and usage screencasts
10. **RVA Publishing Automation** (15-20 hours) - Direct vocabulary publishing to RVA

**Detailed specifications** for all enhancements available in `planning/FAIR4RS-documentation-plan.md` (Optional Future Enhancements section).

**Note:** These are optional quality-of-life improvements. The core software is already FAIR4RS compliant and publication-ready as of October 2025.
